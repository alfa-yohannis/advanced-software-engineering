\chapter{Observabilitas Sistem Terdistribusi}

\section{Tujuan Pembelajaran}

Setelah mengikuti sesi ini, peserta diharapkan mampu:
\begin{itemize}
  \item Memahami observabilitas sebagai pendekatan untuk mengenali dan menalar perilaku sistem berdasarkan data runtime, tanpa bergantung pada struktur internal atau kode sumber sistem.
  \item Mengidentifikasi dan menafsirkan metrik, log, dan trace sebagai representasi perilaku sistem terdistribusi yang melibatkan broker, capturer, transformer, dan web server.
  \item Mengaitkan visualisasi observabilitas pada Prometheus dan Grafana dengan kondisi runtime, beban sistem, serta pengalaman pengguna.
\end{itemize}


\section{Konsep Dasar Observabilitas}

\subsection{Observabilitas vs Monitoring}

Monitoring dan observabilitas sering digunakan secara bergantian, namun keduanya merepresentasikan pendekatan yang berbeda dalam memahami perilaku sistem. Monitoring berfokus pada pengawasan kondisi yang telah diketahui sebelumnya, biasanya melalui sekumpulan metrik atau ambang batas yang telah ditentukan. Pendekatan ini menjawab pertanyaan apakah sistem berada dalam kondisi normal atau tidak.

Observabilitas, di sisi lain, berfokus pada kemampuan untuk menalar kondisi internal sistem berdasarkan sinyal eksternal yang dihasilkan selama sistem berjalan. Tujuannya bukan hanya mendeteksi bahwa terjadi masalah, tetapi juga memahami mengapa dan bagaimana masalah tersebut muncul, termasuk dalam situasi yang sebelumnya tidak diprediksi.

Dalam konteks sistem terdistribusi yang terdiri dari broker, capturer, transformer, dan web server, monitoring umumnya menampilkan indikator terpisah pada setiap komponen, seperti penggunaan sumber daya atau status layanan. Pendekatan ini berguna untuk mendeteksi gangguan lokal, namun sering kali tidak cukup untuk menjelaskan perilaku sistem secara menyeluruh ketika terjadi keterlambatan, penumpukan data, atau kegagalan parsial.

Observabilitas memungkinkan pengamatan perilaku lintas komponen dengan mengaitkan metrik, log, dan trace sebagai satu kesatuan. Melalui korelasi data runtime yang dikumpulkan oleh Prometheus dan divisualisasikan melalui Grafana, peserta dapat menelusuri aliran peristiwa dari satu komponen ke komponen lain, serta memahami dampaknya terhadap respons sistem secara keseluruhan.

Dengan demikian, monitoring dapat dipandang sebagai alat untuk menjawab pertanyaan \emph{``apa yang salah''}, sedangkan observabilitas berperan untuk menjawab \emph{``mengapa hal tersebut terjadi''}. Pendekatan observabilitas menjadi penting dalam sistem modern yang bersifat dinamis, terdistribusi, dan sulit dipahami hanya melalui inspeksi statis atau asumsi desain awal.

\subsection{Metrik, Log, dan Trace}

Metrik, log, dan trace merupakan tiga jenis sinyal utama dalam observabilitas yang merepresentasikan perilaku sistem dari sudut pandang yang berbeda. Ketiganya saling melengkapi dan digunakan bersama untuk membangun pemahaman yang utuh terhadap dinamika sistem saat berjalan.

Metrik menyajikan pengukuran numerik teragregasi yang menggambarkan kondisi sistem dalam rentang waktu tertentu. Contoh metrik meliputi laju pemrosesan, latensi, tingkat kesalahan, dan penggunaan sumber daya. Dalam sistem yang terdiri dari broker, capturer, transformer, dan web server, metrik memungkinkan pengamatan tren seperti peningkatan beban, antrian yang menumpuk, atau penurunan throughput tanpa harus melihat peristiwa individual.

Log merekam peristiwa diskret yang terjadi selama eksekusi sistem, biasanya dalam bentuk pesan berurutan yang merefleksikan aktivitas internal komponen. Log memberikan konteks yang lebih kaya dibandingkan metrik, seperti urutan kejadian, kondisi tertentu, atau keputusan yang diambil oleh sistem. Pada sistem terdistribusi, log membantu menjelaskan apa yang sedang dilakukan oleh masing-masing komponen pada saat tertentu, terutama ketika terjadi kegagalan atau perilaku tidak normal.

Trace merepresentasikan alur eksekusi suatu permintaan atau data saat melewati beberapa komponen sistem. Melalui trace, satu peristiwa dapat ditelusuri dari awal hingga akhir, misalnya dari capturer ke broker, diteruskan ke transformer, hingga akhirnya disajikan oleh web server. Trace memberikan gambaran hubungan sebab-akibat antar komponen serta memungkinkan identifikasi titik keterlambatan atau bottleneck dalam alur sistem.

Dalam praktik observabilitas, ketiga sinyal ini tidak digunakan secara terpisah. Metrik sering digunakan untuk mendeteksi adanya anomali, log digunakan untuk memberikan konteks terhadap anomali tersebut, dan trace digunakan untuk menelusuri jalur eksekusi yang menyebabkan kondisi tersebut muncul. Kombinasi ini memungkinkan pemahaman perilaku sistem secara menyeluruh, bahkan ketika masalah yang terjadi tidak pernah didefinisikan sebelumnya.

Dengan memanfaatkan metrik, log, dan trace secara bersamaan, observabilitas memungkinkan peserta untuk berpindah dari pengamatan permukaan menuju pemahaman mendalam terhadap dinamika runtime sistem terdistribusi.


\subsection{Observabilitas sebagai Pendekatan Bottom-Up}

Pendekatan bottom-up dalam observabilitas menekankan pemahaman sistem yang dimulai dari perilaku nyata yang muncul selama eksekusi, bukan dari asumsi desain atau spesifikasi awal. Dalam pendekatan ini, sistem dipahami melalui sinyal runtime yang dihasilkannya, seperti metrik, log, dan trace, yang merefleksikan interaksi aktual antar komponen.

Berbeda dengan pendekatan top-down yang berangkat dari arsitektur, diagram, atau dokumentasi desain, pendekatan bottom-up mengajak peserta untuk terlebih dahulu mengamati apa yang benar-benar terjadi ketika sistem dijalankan. Hal ini menjadi penting terutama pada sistem terdistribusi yang kompleks, di mana perilaku aktual sering kali menyimpang dari ekspektasi desain akibat beban, kegagalan parsial, atau interaksi non-linear antar komponen.

Dalam sistem yang melibatkan broker, capturer, transformer, dan web server, pendekatan bottom-up memungkinkan pengamatan aliran data dan peristiwa tanpa harus memahami implementasi internal masing-masing komponen. Peserta dapat mulai dengan mengamati metrik seperti laju data masuk, latensi pemrosesan, atau tingkat kesalahan, kemudian menelusuri log dan trace untuk memahami hubungan sebab-akibat di balik perubahan yang teramati.

Pendekatan ini juga mendorong eksplorasi sistem secara iteratif. Pengamatan awal terhadap dashboard atau grafik metrik dapat memunculkan pertanyaan baru, yang selanjutnya dijawab dengan memperkaya konteks melalui log dan trace. Proses ini membentuk siklus observasi dan penalaran yang berulang, di mana pemahaman sistem berkembang seiring dengan meningkatnya kedalaman pengamatan.

Dengan menempatkan data runtime sebagai sumber utama pengetahuan, observabilitas sebagai pendekatan bottom-up membantu membangun intuisi tentang perilaku sistem yang bersifat dinamis dan kontekstual. Pendekatan ini sangat relevan untuk sistem modern yang terus berevolusi, berskala besar, dan tidak dapat sepenuhnya dipahami hanya melalui analisis statis atau dokumentasi desain.




\section{Arsitektur Sistem Studi Kasus}


\begin{figure}[htbp]
\centering
\scalebox{0.9}{
\begin{tikzpicture}[
  font=\small\bfseries,
  node distance=9mm and 12mm,
  box/.style={
    draw,
    rounded corners=2mm,
    align=center,
    minimum width=32mm,
    minimum height=9mm,
    fill=blue!10
  },
  brokerbox/.style={box,fill=orange!15},
  webbox/.style={box,fill=green!12},
  browserbox/.style={box,fill=gray!12},
  extbox/.style={box,fill=purple!10},
  arrow/.style={-Latex,thick},
  dottedarrow/.style={-Latex,thick,dotted}
]

% Core nodes (note: Prometheus service embedded in each node)
\node[box ] (transformer) {Transformer\\(Node B)\\{\scriptsize Prometheus service}};
\node[box,left=of transformer, xshift=-10mm] (capturer) {Capturer\\(Node A)\\{\scriptsize Prometheus service}};
\node[brokerbox, below=of capturer, yshift=-5mm] (broker) {Broker\\(XSUB / XPUB)\\{\scriptsize Prometheus service}};

\node[browserbox,below=of broker] (browser) {Browser};
\node[webbox,right=of browser, xshift=10mm](web) {Web Server\\(Node C)\\{\scriptsize Prometheus service}};

% External observability nodes
\node[extbox,right=of web] (grafana) {Grafana};
\node[extbox,above=of grafana, yshift=10mm] (prom) {Prometheus};

% Data flow arrows
\draw[arrow] (capturer) -- node[above, xshift=-8mm]{PUB \texttt{raw}} (broker);
\draw[arrow] (broker) -- node[above, yshift=-5mm, xshift=8mm]{SUB \texttt{raw}} (transformer);
\draw[arrow] (transformer.south) |- node[below]{PUB \texttt{processed}} (broker.east);
\draw[arrow] (broker) -- node[right,xshift=3mm]{SUB \texttt{processed}} (web.north);
\draw[arrow] (web.west) -- node[above]{HTTP MJPEG} (browser.east);

% Observability (scraping + visualization)
\draw[dottedarrow] (capturer.south) -- (prom);
\draw[dottedarrow] (broker) -- (prom);
\draw[dottedarrow] (transformer.east) -- (prom);
\draw[dottedarrow] (web.east) -- (prom);
\draw[arrow] (prom) -- (grafana);

\end{tikzpicture}
}
\caption{Arsitektur aliran data menggunakan komunikasi publish--subscribe berbasis broker, dengan layanan Prometheus tertanam pada setiap node serta Prometheus dan Grafana sebagai komponen observabilitas eksternal}
\label{fig:broker-pubsub-observability}
\end{figure}

Pada Gambar~\ref{fig:broker-pubsub-observability} ditunjukkan arsitektur sistem studi kasus yang digunakan untuk mengeksplorasi perilaku sistem terdistribusi berbasis komunikasi \emph{publish--subscribe}. Arsitektur ini terdiri dari beberapa node utama yang berinteraksi melalui sebuah \emph{message broker}, serta dilengkapi dengan mekanisme observabilitas yang terintegrasi.

Node \textbf{Capturer (Node A)} berperan sebagai sumber data awal. Node ini menghasilkan data mentah (\emph{raw data}) dan mempublikasikannya ke broker menggunakan mekanisme \emph{publish}. Dengan pendekatan ini, produsen data tidak berkomunikasi langsung dengan konsumen, melainkan melalui perantara.

Node \textbf{Broker} berfungsi sebagai pusat pertukaran data dengan pola komunikasi \emph{XSUB/XPUB}. Broker menerima data mentah dari capturer dan mendistribusikannya kepada node yang melakukan \emph{subscribe}. Pola ini memungkinkan pemisahan yang jelas antara pengirim dan penerima data, serta mendukung skalabilitas sistem.

Node \textbf{Transformer (Node B)} melakukan \emph{subscribe} terhadap data mentah dari broker untuk kemudian memprosesnya. Data hasil pemrosesan (\emph{processed data}) dipublikasikan kembali ke broker, sehingga dapat digunakan oleh komponen lain tanpa ketergantungan langsung.

Node \textbf{Web Server (Node C)} melakukan \emph{subscribe} terhadap data hasil pemrosesan dari broker dan menyajikannya kepada pengguna. Penyajian data dilakukan melalui antarmuka web menggunakan protokol HTTP dalam bentuk aliran MJPEG.

Interaksi pengguna direpresentasikan oleh \textbf{Browser}, yang mengakses web server untuk melihat hasil akhir pemrosesan. Dari sudut pandang pengguna, sistem tampak sebagai layanan web, meskipun terdiri dari beberapa komponen terdistribusi di belakang layar.

Setiap node aplikasi dilengkapi dengan \textbf{layanan Prometheus yang tertanam} untuk mengekspor metrik runtime. Metrik-metrik ini mencerminkan perilaku aktual sistem selama eksekusi. Prometheus eksternal bertugas mengumpulkan metrik dari seluruh node melalui mekanisme \emph{scraping}.

Data observabilitas yang telah dikumpulkan selanjutnya divisualisasikan melalui \textbf{Grafana}. Dashboard Grafana memungkinkan pemantauan dan analisis perilaku sistem secara menyeluruh, serta mendukung pendekatan observabilitas berbasis pengamatan langsung terhadap data runtime.

Arsitektur ini dirancang untuk mendukung pendekatan observabilitas \emph{bottom-up}, di mana pemahaman sistem dibangun dari perilaku nyata yang teramati selama sistem berjalan, sebelum dilakukan analisis struktur atau optimasi desain.


\section{Prometheus}

Prometheus merupakan sistem pemantauan dan pengumpulan metrik yang dirancang untuk sistem terdistribusi dan dinamis. Dalam studi kasus ini, Prometheus digunakan sebagai komponen observabilitas terpusat yang mengumpulkan metrik runtime dari setiap node aplikasi, yaitu Capturer, Broker, Transformer, dan Web Server.

Prometheus menerapkan model \emph{pull-based}, di mana server Prometheus secara periodik melakukan pengambilan (\emph{scraping}) metrik dari endpoint yang diekspos oleh masing-masing node. Setiap node aplikasi menjalankan layanan Prometheus tertanam yang mengekspor metrik runtime tanpa mengganggu alur pemrosesan data utama.

Metrik yang dikumpulkan disimpan dalam bentuk \emph{time-series data}, sehingga memungkinkan analisis perilaku sistem sepanjang waktu, termasuk pengamatan tren, lonjakan beban, latensi, serta indikasi kegagalan parsial. Dalam arsitektur ini, Prometheus tidak terlibat langsung dalam alur data aplikasi, melainkan berfungsi murni sebagai mekanisme observasi.

Pada demo ini, Prometheus tidak diinstal secara manual pada sistem operasi host. Prometheus dijalankan sebagai \emph{container} menggunakan \emph{Docker image} resmi yang di-\emph{pull} dari repositori. Pendekatan ini memberikan lingkungan runtime yang konsisten, mudah direproduksi, dan terisolasi dari node aplikasi.

Dengan menjalankan Prometheus sebagai container, konfigurasi scraping dan penyimpanan metrik dapat dikelola secara terpisah dari kode aplikasi. Selain itu, Prometheus dapat dengan mudah dijalankan, dihentikan, atau dikonfigurasi ulang tanpa memengaruhi proses utama sistem studi kasus.

Untuk mendukung ekspor metrik dari aplikasi berbasis Python, digunakan library \texttt{prometheus-client}. Library ini memungkinkan setiap node menyediakan endpoint metrik yang dapat diakses oleh Prometheus eksternal.

\begin{lstlisting}[language=bash]
pip install prometheus-client
\end{lstlisting}

Dengan pendekatan ini, setiap node aplikasi mengekspor sinyal runtime secara mandiri, sementara Prometheus berperan sebagai pengumpul data terpusat yang berjalan sebagai layanan terpisah. Pemisahan peran ini mendukung observabilitas berbasis data runtime dan memungkinkan analisis perilaku sistem secara \emph{bottom-up} sebelum dilakukan penalaran terhadap struktur atau desain sistem.



\section{Grafana}

Grafana merupakan platform visualisasi data yang digunakan untuk menampilkan dan menganalisis metrik yang dikumpulkan oleh Prometheus. Dalam studi kasus ini, Grafana berperan sebagai lapisan presentasi yang memungkinkan pengamatan perilaku sistem secara intuitif melalui \emph{dashboard}.

Grafana tidak mengumpulkan data secara langsung dari node aplikasi. Grafana berfungsi sebagai klien yang melakukan kueri ke Prometheus untuk mengambil data metrik yang telah tersimpan dalam bentuk \emph{time-series}. Pemisahan peran ini memastikan bahwa visualisasi tidak memengaruhi mekanisme pengumpulan maupun pemrosesan data utama sistem.

Pada demo ini, Grafana tidak dipasang secara manual di sistem operasi host. Sebaliknya, Grafana dijalankan sebagai \emph{container} dengan memanfaatkan \emph{Docker image} resmi yang di-\emph{pull} dari repositori. Pendekatan ini memberikan lingkungan runtime yang konsisten, dapat direproduksi, dan mudah dikonfigurasi ulang, terutama dalam konteks eksperimen dan pembelajaran.

Dengan menjalankan Grafana sebagai container, proses penyebaran dan penghentian layanan visualisasi dapat dilakukan tanpa memengaruhi node aplikasi maupun server Prometheus. Selain itu, konfigurasi Grafana, seperti koneksi ke Prometheus dan definisi dashboard, dapat dikelola secara terpisah dari kode aplikasi.

Melalui dashboard Grafana, metrik dari berbagai node seperti Capturer, Broker, Transformer, dan Web Server dapat ditampilkan secara bersamaan. Visualisasi ini memungkinkan perbandingan perilaku antar node, identifikasi pola beban, serta pengamatan perubahan kondisi sistem dari waktu ke waktu.

Dalam konteks pembelajaran, penggunaan Grafana sebagai container juga memperkuat pemahaman bahwa komponen observabilitas dapat diperlakukan sebagai layanan terpisah. Peserta dapat fokus pada interpretasi perilaku sistem melalui dashboard tanpa harus berurusan dengan kompleksitas instalasi perangkat lunak secara manual.

Dengan demikian, Grafana berfungsi sebagai lapisan visualisasi observabilitas yang fleksibel, terisolasi, dan mudah direproduksi, sejalan dengan tujuan demo untuk menekankan observabilitas berbasis data runtime.


\section{OpenTelemetry}

OpenTelemetry merupakan standar terbuka untuk pengumpulan dan representasi telemetri yang mencakup metrik, log, dan trace. Dalam konteks studi kasus ini, OpenTelemetry diposisikan sebagai kerangka konseptual yang menyatukan berbagai jenis sinyal observabilitas, terutama untuk memahami alur eksekusi lintas node dalam sistem terdistribusi.

Berbeda dengan Prometheus yang berfokus pada pengumpulan metrik berbasis \emph{pull}, OpenTelemetry menekankan konsistensi semantik dalam pembangkitan dan propagasi data telemetri. OpenTelemetry menyediakan model data dan mekanisme instrumentasi yang memungkinkan peristiwa runtime direpresentasikan secara seragam, terlepas dari bahasa pemrograman atau platform yang digunakan.

Dalam arsitektur studi kasus ini, OpenTelemetry relevan terutama untuk konsep \emph{distributed tracing}. Melalui trace, satu alur pemrosesan data dapat diikuti sejak data dihasilkan oleh Capturer, diteruskan melalui Broker, diproses oleh Transformer, hingga akhirnya disajikan oleh Web Server. Setiap tahapan direpresentasikan sebagai \emph{span} yang saling terhubung dalam satu konteks trace.

Penggunaan OpenTelemetry membantu mengaitkan metrik kuantitatif dengan konteks eksekusi yang lebih kaya. Ketika terjadi lonjakan latensi atau penurunan throughput yang terdeteksi melalui metrik Prometheus, trace yang dihasilkan melalui OpenTelemetry dapat digunakan untuk menelusuri titik keterlambatan atau bottleneck pada node tertentu.

Dalam demo ini, OpenTelemetry tidak selalu diaktifkan secara penuh sebagai sistem terpisah, melainkan diperkenalkan sebagai kerangka konseptual untuk memahami relasi antara metrik, log, dan trace. Pendekatan ini menekankan bahwa observabilitas modern tidak hanya bergantung pada satu jenis sinyal, tetapi pada kombinasi berbagai bentuk telemetri yang saling melengkapi.

Dengan memperkenalkan OpenTelemetry, peserta didorong untuk memahami observabilitas sebagai praktik lintas lapisan, yang menghubungkan sinyal runtime dengan alur eksekusi sistem terdistribusi. Pemahaman ini memperkuat pendekatan observabilitas \emph{bottom-up}, di mana sistem dianalisis melalui jejak perilaku aktual yang muncul selama eksekusi.



\section{Eksperimen Observabilitas}

Bagian ini menjelaskan eksperimen observabilitas yang dilakukan menggunakan sistem studi kasus berbasis arsitektur broker--capturer--transformer--web server. Seluruh kode sumber eksperimen tersedia secara terbuka pada repositori berikut:

\noindent\url{https://github.com/alfa-yohannis/advanced-software-engineering/tree/main/projects/session-02/python}

Eksperimen ini bertujuan untuk memperlihatkan bagaimana perilaku sistem terdistribusi dapat diamati melalui data runtime, tanpa harus melakukan analisis kode atau desain secara langsung. Fokus utama eksperimen adalah integrasi metrik, orkestrasi layanan observabilitas, serta interpretasi hasil pengamatan.

\subsection{Tahapan Eksperimen}

Eksperimen observabilitas dilakukan melalui beberapa tahapan berikut:

\begin{enumerate}
  \item \textbf{Instalasi dan Konfigurasi Prometheus}  

Prometheus ditambahkan sebagai komponen observabilitas eksternal yang bertugas mengumpulkan metrik runtime dari seluruh node aplikasi. Prometheus dijalankan sebagai container dan dikonfigurasi untuk melakukan \emph{scraping} terhadap endpoint metrik yang diekspor oleh masing-masing node.

Untuk memungkinkan setiap node aplikasi berbasis Python mengekspor metrik, digunakan library \texttt{prometheus-client}. Library ini dipasang pada seluruh node aplikasi, baik Capturer, Broker, Transformer, maupun Web Server.

\begin{lstlisting}[language=bash]
pip install prometheus-client
\end{lstlisting}

Setelah library terpasang, setiap node menyediakan endpoint \texttt{/metrics} yang berisi metrik runtime dan dapat diakses oleh Prometheus eksternal. Endpoint ini tidak memengaruhi alur data utama sistem dan hanya berfungsi sebagai sumber observasi.

Prometheus dijalankan sebagai layanan terpisah menggunakan Docker. Konfigurasi ini memungkinkan Prometheus mengumpulkan metrik dari seluruh node tanpa harus diinstal langsung pada host sistem.


Pendekatan ini memastikan bahwa mekanisme observabilitas terisolasi dari alur data utama aplikasi, namun tetap mampu merekam perilaku sistem secara menyeluruh untuk keperluan analisis dan eksperimen observabilitas.


  \item \textbf{Pembaruan Kode pada Seluruh Node Aplikasi}  

Kode pada node Capturer, Broker, Transformer, dan Web Server diperbarui untuk mengekspor metrik runtime menggunakan library \texttt{prometheus-client}. Pola implementasi yang digunakan pada demo ini adalah menjalankan HTTP endpoint \texttt{/metrics} pada port yang berbeda untuk setiap node, sehingga Prometheus eksternal dapat melakukan \emph{scraping} secara terpisah per komponen.

Selain itu, ditambahkan \emph{shared library} metrik untuk memastikan konsistensi definisi metrik antar node, misalnya konsistensi penamaan metrik, label yang digunakan, serta tipe metrik (\texttt{Counter}, \texttt{Gauge}, \texttt{Histogram}). Shared library ini bertujuan mencegah duplikasi definisi metrik dan mengurangi risiko inkonsistensi antar node.

Berikut contoh struktur minimal shared library metrik (misal: \texttt{shared/observability.py}):

\begin{lstlisting}[style=PythonStyle]
# shared/observability.py
from prometheus_client import Counter, Gauge, Histogram

# Throughput / volume counters
frames_in_total = Counter(
    "frames_in_total",
    "Total frames received/consumed by a node",
    ["service"]
)

frames_out_total = Counter(
    "frames_out_total",
    "Total frames produced/published by a node",
    ["service"]
)

# Runtime gauges
queue_size = Gauge(
    "queue_size",
    "Approximate queue size (if applicable)",
    ["service", "queue"]
)

# Latency histograms (seconds)
processing_seconds = Histogram(
    "processing_seconds",
    "Processing latency per frame/message in seconds",
    ["service", "stage"]
)
\end{lstlisting}

Setiap node kemudian mengimpor shared library ini dan menjalankan server metrik. Contoh berikut menunjukkan pola umum yang dapat digunakan pada seluruh node:

\begin{lstlisting}[style=PythonStyle]
# contoh potongan umum pada node (capturer/broker/transformer/web)
from prometheus_client import start_http_server
from shared.metrics2 import frames_in_total, frames_out_total, processing_seconds

SERVICE = "capturer"  # ganti sesuai node: broker/transformer/web_server

def init_metrics_server(metrics_port: int) -> None:
    # Menjalankan endpoint /metrics pada port tertentu
    start_http_server(metrics_port)

# Panggil sekali pada startup node
init_metrics_server(9101)  # contoh untuk capturer
\end{lstlisting}

Dengan pola ini, ekspor metrik menjadi seragam: setiap node menjalankan endpoint \texttt{/metrics} dan menggunakan definisi metrik yang sama melalui shared library.

\item \textbf{Penambahan Metrik Runtime}  

Setiap node dilengkapi dengan metrik tambahan yang merepresentasikan perilaku utama sistem, seperti jumlah frame yang diproses, laju publikasi data, serta aktivitas pemrosesan dan penyajian data. Metrik dipilih agar mudah dikaitkan dengan fenomena runtime pada sistem terdistribusi, misalnya throughput, latensi, serta indikasi penumpukan (backpressure) atau bottleneck.

Berikut contoh penambahan metrik pada masing-masing node (cuplikan minimal):

\begin{enumerate}
  \item \textbf{Capturer (Node A):} menghitung frame yang dihasilkan dan waktu akuisisi/publish.
\begin{lstlisting}[style=PythonStyle]
# di loop utama capturer
from time import time
from shared.metrics2 import frames_out_total, processing_seconds

t0 = time()
# ... capture frame ...
# ... publish raw frame ...
frames_out_total.labels(service="capturer").inc()
processing_seconds.labels(service="capturer", stage="capture_publish").observe(time() - t0)
\end{lstlisting}

  \item \textbf{Broker:} menghitung pesan yang diteruskan (raw/processed) dan (opsional) ukuran antrean internal jika ada.
\begin{lstlisting}[style=PythonStyle]
from shared.metrics2 import frames_in_total, frames_out_total

# saat menerima raw dari capturer
frames_in_total.labels(service="broker").inc()

# saat meneruskan ke subscriber (raw atau processed)
frames_out_total.labels(service="broker").inc()
\end{lstlisting}

  \item \textbf{Transformer (Node B):} menghitung frame yang dikonsumsi dan yang diproduksi, serta latensi pemrosesan transformasi.
\begin{lstlisting}[style=PythonStyle]
from time import time
from shared.metrics2 import frames_in_total, frames_out_total, processing_seconds

# saat menerima raw
frames_in_total.labels(service="transformer").inc()

t0 = time()
# ... transform frame ...
frames_out_total.labels(service="transformer").inc()
processing_seconds.labels(service="transformer", stage="transform").observe(time() - t0)
\end{lstlisting}

  \item \textbf{Web Server (Node C):} menghitung frame yang disajikan dan (opsional) latensi penyajian/streaming.
\begin{lstlisting}[style=PythonStyle]
from shared.metrics2 import frames_in_total, frames_out_total

# saat menerima processed dari broker
frames_in_total.labels(service="web_server").inc()

# saat frame dipublikasikan ke stream MJPEG (per frame terkirim)
frames_out_total.labels(service="web_server").inc()
\end{lstlisting}
\end{enumerate}

Dengan penambahan metrik tersebut, peserta dapat melakukan pengamatan lintas node: misalnya membandingkan laju \texttt{frames\_out\_total} pada Capturer dengan \texttt{frames\_in\_total} pada Transformer, atau mengamati distribusi \texttt{processing\_seconds} untuk mengidentifikasi bottleneck pemrosesan. Kombinasi metrik ini juga memudahkan korelasi antara gejala yang terlihat di dashboard dengan peristiwa runtime pada sistem.


  \item \textbf{Penambahan Node Prometheus dan Grafana serta Konfigurasi Docker Compose}  

Dua node baru ditambahkan ke dalam arsitektur sistem, yaitu \textbf{Prometheus} sebagai pengumpul metrik dan \textbf{Grafana} sebagai lapisan visualisasi. Keduanya dijalankan sebagai container terpisah dari node aplikasi, sehingga mekanisme observabilitas tetap terisolasi dari alur pemrosesan data utama.

Berkas \texttt{docker-compose.yml} diperbarui untuk menyertakan layanan Prometheus dan Grafana, serta mendefinisikan relasi jaringan antar seluruh container. Dengan pendekatan ini, seluruh sistem (node aplikasi + observabilitas) dapat dijalankan secara terorkestrasi melalui satu perintah.


Berikut cuplikan konfigurasi layanan \texttt{prometheus} dan \texttt{grafana} pada \texttt{docker-compose.yml}. Prometheus memuat konfigurasi \texttt{prometheus.yml} melalui \emph{volume mount}, sedangkan Grafana disiapkan untuk berjalan sebagai container visualisasi.

\begin{lstlisting}[language=bash]
services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
    depends_on:
      - capturer
      - broker
      - transformer
      - web

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
\end{lstlisting}

Konfigurasi di atas mengasumsikan node aplikasi (capturer, broker, transformer, web\_server) telah didefinisikan sebagai layanan lain dalam file \texttt{docker-compose.yml} yang sama, serta setiap node telah mengekspor endpoint \texttt{/metrics} pada port masing-masing.


Berkas \texttt{prometheus.yml} mendefinisikan interval scraping serta daftar target endpoint metrik dari seluruh node aplikasi. Karena Prometheus berjalan di dalam jaringan Docker Compose, target diakses menggunakan \textbf{nama service} (bukan \texttt{localhost}).

\begin{lstlisting}[language=bash]
global:
  scrape_interval: 5s
  evaluation_interval: 5s

scrape_configs:
  - job_name: "capturer"
    static_configs:
      - targets: ["capturer:9101"]

  - job_name: "broker"
    static_configs:
      - targets: ["broker:9102"]

  - job_name: "transformer"
    static_configs:
      - targets: ["transformer:9103"]

  - job_name: "web_server"
    static_configs:
      - targets: ["web:9104"]
\end{lstlisting}

Dengan konfigurasi ini, Prometheus secara periodik mengumpulkan metrik dari endpoint masing-masing node dan menyimpannya sebagai \emph{time-series}. Pengumpulan metrik dilakukan tanpa mengganggu alur data utama sistem.



Grafana dikonfigurasi agar terhubung ke Prometheus sebagai \emph{data source}. Karena Grafana berjalan dalam jaringan Docker Compose yang sama, alamat yang digunakan adalah \texttt{http://prometheus:9090} (nama service Prometheus di dalam jaringan Docker), bukan \texttt{http://localhost:9090}.

Konfigurasi data source dapat dilakukan melalui antarmuka Grafana:
\begin{enumerate}
  \item Buka Grafana pada \url{http://localhost:3000}.
  \item Masuk menggunakan kredensial default (umumnya \texttt{admin/admin}, kemudian diminta mengganti kata sandi).
  \item Pilih \texttt{Connections} $\rightarrow$ \texttt{Data sources} $\rightarrow$ \texttt{Add data source}.
  \item Pilih tipe \texttt{Prometheus}.
  \item Isi URL menjadi \texttt{http://prometheus:9090}.
  \item Klik \texttt{Save \& Test} untuk memastikan koneksi berhasil.
\end{enumerate}

Setelah data source aktif, metrik yang telah dikumpulkan Prometheus dapat divisualisasikan langsung melalui dashboard Grafana, sehingga peserta dapat melakukan observasi perilaku sistem secara \emph{bottom-up} berdasarkan data runtime.

\end{enumerate}

\subsection{Titik Observasi dan Antarmuka Sistem}

Setelah seluruh layanan dijalankan, eksperimen observabilitas dapat diamati melalui beberapa endpoint berikut:

\begin{itemize}
  \item \textbf{Antarmuka Utama Aplikasi}
    \begin{itemize}
      \item \url{http://localhost:8000/}  
            Menampilkan antarmuka web utama.
      \item \url{http://localhost:8000/stream.mjpg}  
            Menyediakan aliran video MJPEG hasil pemrosesan sistem.
    \end{itemize}

  \item \textbf{Endpoint Metrik Runtime}
    \begin{itemize}
      \item Capturer: \url{http://localhost:9101/metrics}
      \item Broker: \url{http://localhost:9102/metrics}
      \item Transformer: \url{http://localhost:9103/metrics}
      \item Web Server: \url{http://localhost:9104/metrics}
    \end{itemize}

  \item \textbf{Layanan Observabilitas}
    \begin{itemize}
      \item Prometheus: \url{http://localhost:9090}  
            Digunakan untuk melakukan kueri dan eksplorasi metrik secara langsung.
      \item Grafana: \url{http://localhost:3000}  
            Digunakan untuk memvisualisasikan metrik melalui dashboard.
    \end{itemize}
\end{itemize}

Melalui eksperimen ini, peserta dapat mengamati bagaimana perubahan beban, alur data, dan aktivitas pemrosesan pada masing-masing node tercermin secara langsung pada metrik dan visualisasi. Pendekatan ini menegaskan peran observabilitas sebagai sarana utama untuk memahami perilaku sistem secara \emph{bottom-up} berdasarkan data runtime.


\section{Hasil Eksperimen}

Bagian ini menyajikan hasil eksperimen observabilitas yang diperoleh dari sistem studi kasus berbasis pipeline Capturer--Broker--Transformer--Web Server. Hasil eksperimen dirangkum dalam dua tabel utama yang merepresentasikan karakteristik komponen pipeline dan dampak peningkatan jumlah klien pada sisi penyajian data.

\subsection*{Analisis Komponen Pipeline}


\begin{table}[htbp]
\centering
\caption{Ringkasan Peran dan Beban Kerja Setiap Komponen Sistem}
\label{tab:pipeline-comparison-simple}
\begin{tabular}{|
p{0.14\linewidth}|
p{0.25\linewidth}|
p{0.16\linewidth}|
p{0.14\linewidth}|
p{0.14\linewidth}|
}
\hline
\textbf{Komponen} &
\textbf{Fungsi Utama} &
\textbf{Jumlah Data Diproses} &
\textbf{Beban CPU} &
\textbf{Penggunaan Memori} \\
\hline
Capturer &
Mengambil video dan mengubahnya menjadi gambar JPEG &
8.473 gambar dihasilkan &
Tinggi (\textasciitilde 54\%) &
Sedang (\textasciitilde 128 MB) \\
\hline
Broker &
Meneruskan data antar komponen (tanpa memproses isi data) &
17.030 pesan diteruskan &
Sangat rendah (\textasciitilde 2\%) &
Rendah (\textasciitilde 32 MB) \\
\hline
Transformer &
Mengolah gambar (decode, ubah ke grayscale, encode ulang) &
8.641 gambar masuk dan keluar &
Sedang (\textasciitilde 21\%) &
Sedang (45--52 MB) \\
\hline
Web Server &
Menyajikan hasil akhir ke pengguna melalui web &
8.819 gambar dikirim ke pengguna &
Sangat rendah (\textasciitilde 0\%) &
Tinggi (\textasciitilde 193 MB) \\
\hline
\end{tabular}
\end{table}


Tabel~\ref{tab:pipeline-comparison-simple} menyajikan ringkasan dan perbandingan karakteristik setiap komponen dalam pipeline pemrosesan data. Setiap komponen menunjukkan pola perilaku runtime yang berbeda sesuai dengan peran fungsionalnya.

Node \textbf{Capturer} berperan sebagai sumber data utama, bertanggung jawab atas pengambilan video dan pengkodean JPEG. Aktivitas ini tercermin dari penggunaan CPU yang relatif tinggi dibandingkan komponen lain, serta konsumsi memori yang stabil. Jumlah frame keluaran menunjukkan laju produksi data yang konsisten selama eksperimen.

Node \textbf{Broker} berfungsi sebagai perantara komunikasi berbasis XSUB/XPUB. Meskipun broker menangani jumlah frame yang besar, penggunaan CPU dan memori relatif rendah. Hal ini menunjukkan bahwa broker terutama melakukan \emph{message forwarding} tanpa operasi pemrosesan data yang berat, sehingga overhead komputasinya minimal.

Node \textbf{Transformer} melakukan operasi pemrosesan yang lebih kompleks, yaitu dekode JPEG, transformasi citra (grayscale), dan enkode ulang. Aktivitas ini tercermin dari penggunaan CPU yang lebih tinggi dibandingkan broker, serta konsumsi memori yang moderat. Jumlah frame masuk dan keluar yang seimbang menunjukkan bahwa tidak terjadi kehilangan data selama proses transformasi.

Node \textbf{Web Server} berfungsi sebagai komponen penyaji hasil akhir melalui HTTP/MJPEG. Konsumsi memori relatif lebih tinggi dibandingkan komponen lain, yang dapat dikaitkan dengan pengelolaan buffer streaming dan koneksi klien. Penggunaan CPU yang rendah pada saat \emph{scraping} metrik menunjukkan bahwa beban utama web server tidak selalu tercermin secara langsung dalam snapshot CPU tunggal, melainkan dipengaruhi oleh pola akses klien.

Secara keseluruhan, tabel ini menunjukkan bahwa observabilitas memungkinkan pemetaan peran fungsional setiap komponen ke karakteristik penggunaan sumber daya yang terukur.

\subsection*{Dampak Jumlah Klien MJPEG}


\begin{table}[htbp]
\centering
\caption{Performa Web Server saat Jumlah Pengguna Bertambah}
\label{tab:webserver_clients_simple}
\begin{tabular}{|
p{0.25\linewidth}|
p{0.15\linewidth}|
p{0.15\linewidth}|
p{0.25\linewidth}|
}
\hline
\textbf{Yang Diukur} &
\textbf{1 Pengguna} &
\textbf{6 Pengguna} &
\textbf{Perubahan} \\
\hline
Jumlah pengguna yang terhubung &
1 &
6 &
Naik 6 kali \\
\hline
Jumlah gambar yang diterima server &
7.070 &
7.517 &
Naik sedikit (+6\%) \\
\hline
Total data yang diterima (MB) &
697 &
741 &
Naik sedikit (+6\%) \\
\hline
Ukuran rata-rata gambar (KB) &
98,6 &
98,6 &
Tidak berubah \\
\hline
Waktu rata-rata gambar sampai ke pengguna (ms) &
30,1 &
30,2 &
Hampir sama \\
\hline
Waktu terlama yang dirasakan pengguna (ms) &
$\le$ 100 &
$\le$ 100 &
Tidak berubah \\
\hline
Penggunaan CPU server (\%) &
1 &
4 &
Naik sedikit \\
\hline
Penggunaan memori server (MB) &
133 &
157 &
Naik 24 MB \\
\hline
Jumlah koneksi/file yang terbuka &
18 &
23 &
Naik 5 \\
\hline
\end{tabular}
\end{table}


Tabel~\ref{tab:webserver_clients_simple} menyajikan perbandingan performa Web Server ketika jumlah klien MJPEG meningkat dari satu menjadi enam browser secara bersamaan. Hasil ini digunakan untuk mengevaluasi skalabilitas dan stabilitas sistem pada sisi penyajian data.

Perlu dicatat bahwa pengambilan cuplikan data untuk skenario enam pengguna dilakukan pada waktu yang sedikit lebih belakangan dibandingkan skenario satu pengguna. Akibatnya, jumlah gambar yang diterima serta total data yang diterima pada skenario enam pengguna tercatat lebih besar. Perbedaan ini mencerminkan durasi pengamatan yang lebih panjang, bukan semata-mata peningkatan laju pemrosesan atau perubahan perilaku sistem.

Peningkatan jumlah klien dari satu menjadi enam tidak menyebabkan perubahan signifikan pada ukuran rata-rata JPEG maupun latensi end-to-end. Nilai latensi rata-rata dan batas atas latensi tetap berada dalam rentang yang sama, menunjukkan bahwa sistem mampu mempertahankan kualitas layanan meskipun jumlah klien meningkat.

Dari sisi sumber daya, peningkatan jumlah klien menyebabkan kenaikan penggunaan CPU dan memori pada Web Server, serta bertambahnya jumlah \emph{file descriptor} terbuka. Namun, peningkatan tersebut bersifat linier dan masih dalam batas yang dapat diterima untuk skenario eksperimen ini. Tidak terlihat adanya gejala degradasi performa yang signifikan atau bottleneck kritis.

Hasil ini mengindikasikan bahwa arsitektur pipeline dan mekanisme penyajian MJPEG yang digunakan cukup efektif untuk menangani peningkatan jumlah klien dalam skala kecil hingga menengah. Observabilitas berbasis metrik memungkinkan interpretasi hasil eksperimen secara lebih akurat, termasuk membedakan dampak peningkatan beban nyata dengan perbedaan waktu pengamatan.



\section{Sudut Pandang Mahasiswa Non-IT}

Bagian ini bertujuan melatih kemampuan membaca dan menalar perilaku sistem berdasarkan representasi visual dan konsep model, tanpa harus memahami kode sumber.

\begin{enumerate}
  \item Amati salah satu dashboard Grafana yang menampilkan metrik sistem.  
  Jelaskan perilaku sistem yang Anda amati menggunakan bahasa sehari-hari.  
  

  \item Identifikasi satu metrik yang menurut Anda paling penting bagi pengguna akhir (misalnya latensi atau jumlah data yang diproses).  
  Jelaskan mengapa metrik tersebut relevan dari sudut pandang pengguna.  
  

  \item Bandingkan dua kondisi sistem (misalnya jumlah pengguna sedikit dan banyak).  
  Apa perubahan yang terlihat, dan apa yang tidak berubah?  
  
\end{enumerate}

\section{Sudut Pandang Mahasiswa IT}

Bagian ini berfokus pada hubungan antara implementasi teknis, metrik runtime, dan representasi model sistem.

\begin{enumerate}
  \item Pilih satu metrik Prometheus yang diekspor oleh salah satu node.  
  Jelaskan bagian kode mana yang bertanggung jawab menghasilkan metrik tersebut.  
  

  \item Buat sketsa sederhana (diagram atau teks) yang memodelkan alur data dari Capturer hingga Web Server.  
  Tandai di mana metrik dihasilkan pada setiap komponen.  
  

  \item Jika Anda diminta menambahkan satu node baru (misalnya \emph{Filter}), metrik apa saja yang perlu ditambahkan agar node tersebut tetap dapat diamati dengan baik?  
	
\end{enumerate}

\section{Diskusi dan Refleksi}

Bagian ini mendorong refleksi lintas disiplin dan pengaitan langsung dengan prinsip Model-Driven Engineering.

\begin{enumerate}
  \item Diskusikan bagaimana dashboard Grafana dapat dipandang sebagai \emph{abstraksi} dari sistem yang sedang berjalan.  
  Apa keuntungan menggunakan abstraksi seperti ini dibandingkan membaca kode atau log secara langsung?

  \item Jelaskan perbedaan antara:
  \begin{itemize}
    \item sistem sebagai implementasi, dan
    \item sistem sebagai model atau representasi yang diamati melalui metrik dan visualisasi.
  \end{itemize}

  \item Refleksikan peran observabilitas dalam tahapan-tahapan rekayasa perangkat lunak (requirements gathering, analysis, desain,  pengujian, deployment, operasi).  
  Menurut Anda, pada tahap mana observabilitas memberikan nilai paling besar?
\end{enumerate}

\section{Ringkasan}

Bab ini memperkenalkan observabilitas sebagai pendekatan untuk memahami sistem terdistribusi melalui data runtime, bukan melalui inspeksi statis terhadap kode atau asumsi desain awal. Peserta mempelajari perbedaan antara monitoring dan observabilitas, serta peran metrik, log, dan trace sebagai sinyal utama untuk menalar perilaku sistem. Melalui pendekatan bottom-up, pemahaman dibangun dari apa yang benar-benar terjadi saat sistem berjalan, termasuk perubahan beban, keterlambatan, penumpukan data, dan dampaknya terhadap pengalaman pengguna.

Studi kasus yang digunakan adalah pipeline terdistribusi berbasis broker dengan komponen Capturer, Broker, Transformer, dan Web Server, yang dilengkapi layanan Prometheus tertanam pada setiap node. Prometheus eksternal mengumpulkan metrik melalui scraping dan Grafana memvisualisasikannya melalui dashboard. Eksperimen menunjukkan bahwa tiap komponen memiliki karakteristik beban kerja berbeda sesuai fungsinya, serta peningkatan jumlah pengguna MJPEG dapat diamati melalui perubahan penggunaan sumber daya tanpa perubahan signifikan pada latensi. Melalui tugas dan refleksi, peserta diajak mengaitkan observabilitas dengan prinsip-prinsip MDE, terutama gagasan bahwa dashboard dan metrik dapat dipandang sebagai model tingkat tinggi yang membantu menjembatani implementasi, eksekusi, dan perbaikan sistem secara iteratif.


